{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stocks Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! CSVs saved in 'E:\\Sakthi\\prasanth\\projects\\stocks\\Scripts\\allstocks_csv'\n"
     ]
    }
   ],
   "source": [
    "#ymal to csv stock-wise\n",
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "#set-input-and-output-directories\n",
    "data_dir = Path(\"E:\\Sakthi\\prasanth\\projects\\stocks\\Scripts\\data\")\n",
    "output_dir = Path(\"allstocks_csv\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "#dictionary-to-store-data-per-stock-symbol\n",
    "stock_data = {}\n",
    "\n",
    "#loop-to-take-all-months\n",
    "for month_folder in data_dir.iterdir():\n",
    "    if month_folder.is_dir():\n",
    "        for yaml_file in month_folder.glob(\"*.yaml\"):\n",
    "            with open(yaml_file, 'r') as f:\n",
    "                try:\n",
    "                    content = yaml.safe_load(f)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {yaml_file.name}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                #handle-both-list-and-single-dict\n",
    "                if isinstance(content, list):\n",
    "                    entries = content\n",
    "                elif isinstance(content, dict):\n",
    "                    entries = [content]\n",
    "                else:\n",
    "                    print(f\"Skipped {yaml_file.name} (unexpected format)\")\n",
    "                    continue\n",
    "\n",
    "                for row_data in entries:\n",
    "                    symbol = row_data.get(\"Ticker\")\n",
    "                    if not symbol:\n",
    "                        continue\n",
    "\n",
    "                    #rows-for-stocks\n",
    "                    row = {\n",
    "                        \"date\": row_data.get(\"date\"),\n",
    "                        \"open\": row_data.get(\"open\"),\n",
    "                        \"close\": row_data.get(\"close\"),\n",
    "                        \"high\": row_data.get(\"high\"),\n",
    "                        \"low\": row_data.get(\"low\"),\n",
    "                        \"volume\": row_data.get(\"volume\"),\n",
    "                    }\n",
    "\n",
    "                    stock_data.setdefault(symbol, []).append(row)\n",
    "\n",
    "#convert-to-dataframe-save-as-.csv\n",
    "for symbol, rows in stock_data.items():\n",
    "    df = pd.DataFrame(rows)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.sort_values(\"date\", inplace=True)\n",
    "    df.to_csv(output_dir / f\"{symbol}.csv\", index=False)\n",
    "\n",
    "print(f\"CSVs saved in '{output_dir.resolve()}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All stocks combined into 'all_stocks_combined.csv'\n"
     ]
    }
   ],
   "source": [
    "#combine all stocks as one csv file\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#path\n",
    "csv_folder = \"E:\\Sakthi\\prasanth\\projects\\stocks\\Scripts\\stocks_csv\"\n",
    "\n",
    "dfs = []\n",
    "\n",
    "#loop through each CSV file in the folder\n",
    "for filename in os.listdir(csv_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        ticker = filename.replace(\".csv\", \"\")\n",
    "        df = pd.read_csv(os.path.join(csv_folder, filename))\n",
    "        df[\"Ticker\"] = ticker  # Add a column for the ticker\n",
    "        dfs.append(df)\n",
    "\n",
    "#concat all dataframes into one\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "#save as single csv file\n",
    "combined_df.to_csv(\"all_stocks.csv\", index=False)\n",
    "\n",
    "print(\"All stocks combined into 'all_stocks.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read\n",
    "import pandas as pd\n",
    "df = pd.read_csv('E:/Sakthi/prasanth/projects/stocks/Scripts/all_stocks.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column-lowercase\n",
    "df.columns = df.columns.str.lower()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date     open    close     high      low   volume    ticker\n",
      "0  2023-10-03  2418.00  2387.25  2424.90  2372.00  2019899  ADANIENT\n",
      "1  2023-10-04  2402.20  2464.95  2502.75  2392.25  2857377  ADANIENT\n",
      "2  2023-10-05  2477.95  2466.35  2486.50  2446.40  1132455  ADANIENT\n",
      "3  2023-10-06  2466.35  2478.10  2514.95  2466.05  1510035  ADANIENT\n",
      "4  2023-10-09  2440.00  2442.60  2459.70  2411.30  1408224  ADANIENT\n"
     ]
    }
   ],
   "source": [
    "#date-only\n",
    "df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to-save\n",
    "df.to_csv('all_stocks.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NaN values Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date     open    close     high      low   volume    ticker\n",
      "0  2023-10-03  2418.00  2387.25  2424.90  2372.00  2019899  ADANIENT\n",
      "1  2023-10-04  2402.20  2464.95  2502.75  2392.25  2857377  ADANIENT\n",
      "2  2023-10-05  2477.95  2466.35  2486.50  2446.40  1132455  ADANIENT\n",
      "3  2023-10-06  2466.35  2478.10  2514.95  2466.05  1510035  ADANIENT\n",
      "4  2023-10-09  2440.00  2442.60  2459.70  2411.30  1408224  ADANIENT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('E:/Sakthi/prasanth/projects/stocks/Scripts/all_stocks.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14200 entries, 0 to 14199\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   date    14200 non-null  object \n",
      " 1   open    14200 non-null  float64\n",
      " 2   close   14200 non-null  float64\n",
      " 3   high    14200 non-null  float64\n",
      " 4   low     14200 non-null  float64\n",
      " 5   volume  14200 non-null  int64  \n",
      " 6   ticker  14200 non-null  object \n",
      "dtypes: float64(4), int64(1), object(2)\n",
      "memory usage: 776.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date      0\n",
       "open      0\n",
       "close     0\n",
       "high      0\n",
       "low       0\n",
       "volume    0\n",
       "ticker    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sector Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             COMPANY         sector                         Symbol\n",
      "0  ADANI ENTERPRISES  MISCELLANEOUS  ADANI ENTERPRISES: ADANIGREEN\n",
      "1  ADANI PORTS & SEZ  MISCELLANEOUS  ADANI PORTS & SEZ: ADANIPORTS\n",
      "2   APOLLO HOSPITALS  MISCELLANEOUS   APOLLO HOSPITALS: APOLLOHOSP\n",
      "3       ASIAN PAINTS         PAINTS       ASIAN PAINTS: ASIANPAINT\n",
      "4          AXIS BANK        BANKING            AXIS BANK: AXISBANK\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dfs = pd.read_csv('E:\\Sakthi\\prasanth\\projects\\stocks\\Scripts\\Sector_data - Sheet1.csv')\n",
    "print(dfs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             company         sector                         symbol\n",
      "0  ADANI ENTERPRISES  MISCELLANEOUS  ADANI ENTERPRISES: ADANIGREEN\n",
      "1  ADANI PORTS & SEZ  MISCELLANEOUS  ADANI PORTS & SEZ: ADANIPORTS\n",
      "2   APOLLO HOSPITALS  MISCELLANEOUS   APOLLO HOSPITALS: APOLLOHOSP\n",
      "3       ASIAN PAINTS         PAINTS       ASIAN PAINTS: ASIANPAINT\n",
      "4          AXIS BANK        BANKING            AXIS BANK: AXISBANK\n"
     ]
    }
   ],
   "source": [
    "#column-lowercase\n",
    "dfs.columns = dfs.columns.str.lower()\n",
    "print(dfs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50 entries, 0 to 49\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   company  50 non-null     object\n",
      " 1   sector   50 non-null     object\n",
      " 2   symbol   50 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.3+ KB\n"
     ]
    }
   ],
   "source": [
    "dfs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company    0\n",
       "sector     0\n",
       "symbol     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             company         sector       symbol\n",
      "0  ADANI ENTERPRISES  MISCELLANEOUS   ADANIGREEN\n",
      "1  ADANI PORTS & SEZ  MISCELLANEOUS   ADANIPORTS\n",
      "2   APOLLO HOSPITALS  MISCELLANEOUS   APOLLOHOSP\n",
      "3       ASIAN PAINTS         PAINTS   ASIANPAINT\n",
      "4          AXIS BANK        BANKING     AXISBANK\n"
     ]
    }
   ],
   "source": [
    "#symbol-match-with-ticker\n",
    "dfs['symbol'] = dfs['symbol'].str.split(':').str[1]\n",
    "print(dfs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ADANIGREEN' ' ADANIPORTS' ' APOLLOHOSP' ' ASIANPAINT' ' AXISBANK'\n",
      " ' BAJAJ-AUTO' ' BAJFINANCE' ' BAJAJFINSV' ' BEL' ' AIRTEL' ' BPCL'\n",
      " ' CIPLA' ' COALINDIA' ' DRREDDY' ' EICHERMOT' ' GRASIM' ' HCLTECH'\n",
      " ' HDFCBANK' ' HDFCLIFE' ' HEROMOTOCO' ' HINDALCO' ' HINDUNILVR'\n",
      " ' ICICIBANK' ' INDUSINDBK' ' INFY' ' IOC' ' ITC' ' JSWSTEEL' ' KOTAKBANK'\n",
      " ' LT' ' M&M' ' MARUTI' ' NESTLEIND' ' NTPC' ' ONGC' ' POWERGRID'\n",
      " ' RELIANCE' ' SBIN' ' SBILIFE' ' SHRIRAMFIN' ' SUNPHARMA' ' TATACONSUMER'\n",
      " ' TATAMOTORS' ' TATASTEEL' ' TCS' ' TECHM' ' TITAN' ' TRENT'\n",
      " ' ULTRACEMCO' ' WIPRO']\n"
     ]
    }
   ],
   "source": [
    "print(dfs['symbol'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADANIGREEN' 'ADANIPORTS' 'APOLLOHOSP' 'ASIANPAINT' 'AXISBANK'\n",
      " 'BAJAJ-AUTO' 'BAJFINANCE' 'BAJAJFINSV' 'BEL' 'AIRTEL' 'BPCL' 'CIPLA'\n",
      " 'COALINDIA' 'DRREDDY' 'EICHERMOT' 'GRASIM' 'HCLTECH' 'HDFCBANK'\n",
      " 'HDFCLIFE' 'HEROMOTOCO' 'HINDALCO' 'HINDUNILVR' 'ICICIBANK' 'INDUSINDBK'\n",
      " 'INFY' 'IOC' 'ITC' 'JSWSTEEL' 'KOTAKBANK' 'LT' 'M&M' 'MARUTI' 'NESTLEIND'\n",
      " 'NTPC' 'ONGC' 'POWERGRID' 'RELIANCE' 'SBIN' 'SBILIFE' 'SHRIRAMFIN'\n",
      " 'SUNPHARMA' 'TATACONSUMER' 'TATAMOTORS' 'TATASTEEL' 'TCS' 'TECHM' 'TITAN'\n",
      " 'TRENT' 'ULTRACEMCO' 'WIPRO']\n"
     ]
    }
   ],
   "source": [
    "#remove-space\n",
    "dfs['symbol'] = dfs['symbol'].str.strip()\n",
    "print(dfs['symbol'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tickers in all_stocks.csv not found in updated_sector.csv: ['ADANIENT' 'BHARTIARTL' 'BRITANNIA' 'TATACONSUM']\n",
      "Tickers in updated_sector.csv not found in all_stocks.csv : ['ADANIGREEN' 'AIRTEL' 'IOC' 'TATACONSUMER']\n"
     ]
    }
   ],
   "source": [
    "#mismatched-symbol-and-ticker\n",
    "mismatched_ticker = df[~df['ticker'].isin(dfs['symbol'])]['ticker'].unique()\n",
    "mismatched_symbol = dfs[~dfs['symbol'].isin(df['ticker'])]['symbol'].unique()\n",
    "\n",
    "print(f\"Tickers in all_stocks.csv not found in updated_sector.csv: {mismatched_ticker}\")\n",
    "print(f\"Symbols in updated_sector.csv not found in all_stocks.csv : {mismatched_symbol}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change-symbols-to-match-the-ticker\n",
    "dfs['symbol'] = dfs['symbol'].replace('ADANIGREEN', 'ADANIENT')\n",
    "dfs['symbol'] = dfs['symbol'].replace('AIRTEL', 'BHARTIARTL')\n",
    "dfs['symbol'] = dfs['symbol'].replace('TATACONSUMER', 'TATACONSUM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADANIENT' 'ADANIPORTS' 'APOLLOHOSP' 'ASIANPAINT' 'AXISBANK' 'BAJAJ-AUTO'\n",
      " 'BAJFINANCE' 'BAJAJFINSV' 'BEL' 'BHARTIARTL' 'BPCL' 'CIPLA' 'COALINDIA'\n",
      " 'DRREDDY' 'EICHERMOT' 'GRASIM' 'HCLTECH' 'HDFCBANK' 'HDFCLIFE'\n",
      " 'HEROMOTOCO' 'HINDALCO' 'HINDUNILVR' 'ICICIBANK' 'INDUSINDBK' 'INFY'\n",
      " 'IOC' 'ITC' 'JSWSTEEL' 'KOTAKBANK' 'LT' 'M&M' 'MARUTI' 'NESTLEIND' 'NTPC'\n",
      " 'ONGC' 'POWERGRID' 'RELIANCE' 'SBIN' 'SBILIFE' 'SHRIRAMFIN' 'SUNPHARMA'\n",
      " 'TATACONSUM' 'TATAMOTORS' 'TATASTEEL' 'TCS' 'TECHM' 'TITAN' 'TRENT'\n",
      " 'ULTRACEMCO' 'WIPRO']\n"
     ]
    }
   ],
   "source": [
    "print(dfs['symbol'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             company     sector      symbol\n",
      "46             TITAN  RETAILING       TITAN\n",
      "47             TRENT  RETAILING       TRENT\n",
      "48  ULTRATECH CEMENT     CEMENT  ULTRACEMCO\n",
      "49             WIPRO   SOFTWARE       WIPRO\n",
      "50         BRITANNIA       FMCG   BRITANNIA\n",
      "      company sector     symbol\n",
      "50  BRITANNIA   FMCG  BRITANNIA\n"
     ]
    }
   ],
   "source": [
    "#add-new-column-for-britannia\n",
    "new_row = {'company': 'BRITANNIA','sector': 'FMCG', 'symbol': 'BRITANNIA'}\n",
    "\n",
    "#create-datafame\n",
    "new_row_df = pd.DataFrame([new_row])\n",
    "\n",
    "#Check-if-the-new-row-already-exists-in-the-DataFrame\n",
    "if not ((dfs['company'] == new_row['company']) & (dfs['symbol'] == new_row['symbol'])).any():\n",
    "    #append-the-new-row-to-the-DataFrame-using-pd.concat\n",
    "    dfs = pd.concat([dfs, new_row_df], ignore_index=True)\n",
    "else:\n",
    "    print(\"Row already exists.\")\n",
    "\n",
    "print(dfs.tail())\n",
    "print(dfs[dfs['company'] == 'BRITANNIA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to-delete-company\n",
    "dfs = dfs[dfs['company'] != 'IOC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADANIENT' 'ADANIPORTS' 'APOLLOHOSP' 'ASIANPAINT' 'AXISBANK' 'BAJAJ-AUTO'\n",
      " 'BAJFINANCE' 'BAJAJFINSV' 'BEL' 'BHARTIARTL' 'BPCL' 'CIPLA' 'COALINDIA'\n",
      " 'DRREDDY' 'EICHERMOT' 'GRASIM' 'HCLTECH' 'HDFCBANK' 'HDFCLIFE'\n",
      " 'HEROMOTOCO' 'HINDALCO' 'HINDUNILVR' 'ICICIBANK' 'INDUSINDBK' 'INFY'\n",
      " 'ITC' 'JSWSTEEL' 'KOTAKBANK' 'LT' 'M&M' 'MARUTI' 'NESTLEIND' 'NTPC'\n",
      " 'ONGC' 'POWERGRID' 'RELIANCE' 'SBIN' 'SBILIFE' 'SHRIRAMFIN' 'SUNPHARMA'\n",
      " 'TATACONSUM' 'TATAMOTORS' 'TATASTEEL' 'TCS' 'TECHM' 'TITAN' 'TRENT'\n",
      " 'ULTRACEMCO' 'WIPRO' 'BRITANNIA']\n"
     ]
    }
   ],
   "source": [
    "print(dfs['symbol'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to-save\n",
    "dfs.to_csv('updated_sector.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To store all_stocks.csv to my Tidb database for Streamlit Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allstocks table created and data uploaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Database connection \n",
    "host = \"gateway01.eu-central-1.prod.aws.tidbcloud.com\"\n",
    "port = 4000\n",
    "user = \"42aq8sKC2dkkKnC.root\"\n",
    "password = \"YOUR PASSWORD\"\n",
    "ssl_ca_path = r\"E:\\Sakthi\\prasanth\\projects\\imdb\\Scripts\\certs\\ca.pem\"\n",
    "ssl_args = f\"?ssl_ca={ssl_ca_path}\"\n",
    "\n",
    "# Temporary engine to create database\n",
    "temp_engine = create_engine(\n",
    "    f\"mysql+pymysql://{user}:{password}@{host}:{port}/information_schema\",\n",
    "    connect_args={\n",
    "        \"ssl\": {\"ca\": ssl_ca_path}\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create stocks database\n",
    "with temp_engine.connect() as conn:\n",
    "    conn.execute(text(\"CREATE DATABASE IF NOT EXISTS stocks\"))\n",
    "\n",
    "# Connect to the stocks database\n",
    "engine = create_engine(\n",
    "    f\"mysql+pymysql://{user}:{password}@{host}:{port}/stocks{ssl_args}\"\n",
    ")\n",
    "\n",
    "# Table Query \n",
    "create_table_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS allstocks (\n",
    "    date TEXT,\n",
    "    open DOUBLE, \n",
    "    close DOUBLE,\n",
    "    high DOUBLE,\n",
    "    low DOUBLE,\n",
    "    volume BIGINT,\n",
    "    ticker TEXT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(create_table_sql))\n",
    "\n",
    "# Load CSV\n",
    "csv_path = \"E:\\\\Sakthi\\\\prasanth\\\\projects\\\\stocks\\\\Scripts\\\\all_stocks.csv\"  \n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Insert into movies table\n",
    "df.to_sql(\n",
    "    name=\"allstocks\",\n",
    "    con=engine,\n",
    "    if_exists=\"append\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"allstocks table created and data uploaded successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stocks (3.11.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
